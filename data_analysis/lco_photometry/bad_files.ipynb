{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da984919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightkurve as lk\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from astropy.io import ascii\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c4d2c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gp\n",
    "# ---\n",
    "gp_data = ascii.read('/Users/mschochet/Desktop/MSU_PHD/lco_aumic/lcs_posttwirl/gp.xls')\n",
    "gp_data.sort(keys='rel_flux_T1')\n",
    "gp_badfiles = [str(val) for val in list(gp_data[:2]['Label'])]\n",
    "\n",
    "gp_cleaned = gp_data[2:] # Remove any frames where AIJ misplaced the aperture and T1 or C2 fluxes are negative/near-0\n",
    "gp_cleaned.sort(keys='J.D.-2400000')\n",
    "gp_d, gp_aum_flux, gp_c1_flux = gp_cleaned['J.D.-2400000'], gp_cleaned['rel_flux_T1'], gp_cleaned['rel_flux_C2']\n",
    "# ---\n",
    "\n",
    "#ip\n",
    "# ---\n",
    "ip_data = ascii.read('/Users/mschochet/Desktop/MSU_PHD/lco_aumic/lcs_posttwirl/ip.xls')\n",
    "ip_data.sort(keys='rel_flux_T1')\n",
    "ip_badfiles = [str(val) for val in list(ip_data[:28]['Label'])]\n",
    "ip_badfiles.append(ip_data[len(ip_data)-1][\"Label\"])\n",
    "\n",
    "ip_cleaned = ip_data[28:len(ip_data)-1] # Same as above but there is a +40 value out of nowhere, don't think that is real\n",
    "ip_cleaned.sort(keys='J.D.-2400000')\n",
    "ip_d, ip_aum_flux, ip_c1_flux = ip_cleaned['J.D.-2400000'], ip_cleaned['rel_flux_T1'], ip_cleaned['rel_flux_C2']\n",
    "# ---\n",
    "\n",
    "#rp\n",
    "# ---\n",
    "rp_data = ascii.read('/Users/mschochet/Desktop/MSU_PHD/lco_aumic/lcs_posttwirl/rp.xls')\n",
    "rp_data.sort(keys='rel_flux_T1')\n",
    "rp_badfiles = [str(val) for val in list(rp_data[:1]['Label'])]\n",
    "\n",
    "rp_cleaned = rp_data[1:] # Same as above but there is only one bad value\n",
    "rp_cleaned.sort(keys='J.D.-2400000')\n",
    "rp_d, rp_aum_flux, rp_c1_flux = rp_cleaned['J.D.-2400000'], rp_cleaned['rel_flux_T1'], rp_cleaned['rel_flux_C2']\n",
    "# ---\n",
    "\n",
    "#U\n",
    "# ---\n",
    "U_data = ascii.read('/Users/mschochet/Desktop/MSU_PHD/lco_aumic/lcs_posttwirl/U.xls')\n",
    "U_data.sort(keys='rel_flux_T1')\n",
    "U_badfiles = [str(val) for val in list(U_data[(len(U_data))-13:]['Label'])]\n",
    "U_badfiles.append(U_data[0][\"Label\"])\n",
    "U_badfiles.append(U_data[1][\"Label\"])\n",
    "\n",
    "\n",
    "U_cleaned = U_data[2:(len(U_data))-13]# Same as above but there are 13 values with substantially too large flux values, and 2 with values too low \n",
    "U_cleaned.sort(keys='J.D.-2400000')\n",
    "U_d, U_aum_flux, U_c1_flux = U_cleaned['J.D.-2400000'], U_cleaned['rel_flux_T1'], U_cleaned['rel_flux_C2']\n",
    "# ---\n",
    "\n",
    "#B\n",
    "# ---\n",
    "B_data = ascii.read('/Users/mschochet/Desktop/MSU_PHD/lco_aumic/lcs_posttwirl/B.xls')\n",
    "\n",
    "B_cleaned = B_data.copy()\n",
    "B_cleaned.sort(keys='J.D.-2400000')\n",
    "B_d, B_aum_flux, B_c1_flux = B_cleaned['J.D.-2400000'], B_cleaned['rel_flux_T1'], B_cleaned['rel_flux_C2']\n",
    "# ---\n",
    "\n",
    "#V\n",
    "# ---\n",
    "V_data = ascii.read('/Users/mschochet/Desktop/MSU_PHD/lco_aumic/lcs_posttwirl/V.xls')\n",
    "\n",
    "V_cleaned = V_data.copy()\n",
    "V_cleaned.sort(keys='J.D.-2400000')\n",
    "V_d, V_aum_flux, V_c1_flux = V_cleaned['J.D.-2400000'], V_cleaned['rel_flux_T1'], V_cleaned['rel_flux_C2']\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0266d0",
   "metadata": {},
   "source": [
    "## Find and save the names of the bad files in each filter (Only works with file drive plugged in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "993ea6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This accounts for files lost by twirl and with bad AIJ values, but to catch the ones that crashed AIJ I need to compare\n",
    "\n",
    "def find_lost(datalist, type, og_badfiles):\n",
    "    \"\"\" Re-solves an LCO photometry WCS in the file header. Takes in a .fits file and returns the same file, with a newly re-solved WCS using twirl's \n",
    "        interface for finding sources and matching to Gaia DR3.\n",
    "\n",
    "    Args:\n",
    "        datalist (pd.DataFrame): list of files with a 'Label\" column\n",
    "        type (str): 'gp', 'ip', 'rp', 'U', 'B', V'\n",
    "        '\n",
    "    \"\"\" \n",
    "    #og_badfiles = [str(val) for val in og_badfiles]\n",
    "    og_badfiles = [str(os.path.basename(val)) for val in og_badfiles]\n",
    "    files_postaij = sorted([str(val) for val in list(datalist['Label'])])\n",
    "    postaij_df = pd.DataFrame({\"files\": files_postaij})\n",
    "    if(type=='U'):\n",
    "        files_preaij = sorted(glob(f\"/Volumes/harddrive/U_notwirl/*.fits.fz\"))\n",
    "    else:\n",
    "        files_preaij = sorted(glob(f\"/Volumes/harddrive/{type}/aligned/*.fits\"))\n",
    "    preaij_df = pd.DataFrame({\"files\": files_preaij})\n",
    "    files_onlyname = [str(os.path.basename(val)) for val in (preaij_df['files'])]\n",
    "    preaij_df2 = pd.DataFrame({\"files\": files_onlyname})\n",
    "    combined = [bool(np.isnan(val)) for val in preaij_df2['files'].value_counts() - postaij_df['files'].value_counts()]\n",
    "    badfiles = [str(val) for val in preaij_df2[combined]['files'].values.tolist()]\n",
    "    print('The number of bad files in the', type, 'filter are:', len(badfiles+og_badfiles))\n",
    "    with open(f'bad_files/bad_{type}files.txt', 'w') as f:\n",
    "        for item in og_badfiles+badfiles:\n",
    "            new_string = item.replace(\"aligned_\", \"\")\n",
    "            f.write(new_string + '\\n')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6805210b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of bad files in the ip filter are: 38\n",
      "The number of bad files in the gp filter are: 2\n",
      "The number of bad files in the rp filter are: 2\n",
      "The number of bad files in the U filter are: 15\n"
     ]
    }
   ],
   "source": [
    "find_lost(ip_data, \"ip\", ip_badfiles)\n",
    "find_lost(gp_data, \"gp\", gp_badfiles)\n",
    "find_lost(rp_data, \"rp\", rp_badfiles)\n",
    "find_lost(U_data, \"U\", U_badfiles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
